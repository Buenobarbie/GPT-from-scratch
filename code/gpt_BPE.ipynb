{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bBo_SCD0dKQ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buenobarbie/GPT-from-scratch/blob/master/gpt_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D6SXTur29oL"
      },
      "source": [
        "# GPT with Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GRnq7cE62sDu",
        "outputId": "00d57f3b-00d8-48f3-99f2-aebc57eb98b6"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSalddn30dKW"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdYcGuNr0dKX",
        "outputId": "f84ae2b1-4621-4aa8-9894-b027188be0e3"
      },
      "outputs": [],
      "source": [
        "MANUAL_SEED = 1337\n",
        "LOAD = False\n",
        "DATASET = True\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "\n",
        "config_path = '../models/config.json'\n",
        "model_path = '../models/model.pth'\n",
        "\n",
        "save_config_path = '../models/config2.json'\n",
        "save_model_path = '../models/mode2l.pth'\n",
        "\n",
        "\n",
        "if LOAD:\n",
        "    # load hyperparameters from json file\n",
        "    with open(config_path) as f:\n",
        "        data = json.load(f)\n",
        "        batch_size = data['batch_size']\n",
        "        block_size = data['block_size']\n",
        "        max_steps = data['max_steps']\n",
        "        learning_rate = data['learning_rate']\n",
        "        eval_iters = data['eval_iters']\n",
        "        n_emb = data['n_emb']\n",
        "        n_layer = data['n_layer']\n",
        "        n_head = data['n_head']\n",
        "        dropout = data['dropout']\n",
        "        vocab_size = data['vocab_size']\n",
        "        stoi = {s:int(i) for s,i in data['stoi'].items()}\n",
        "        itos = {int(i):s for i,s in data['itos'].items()}\n",
        "\n",
        "\n",
        "else:\n",
        "    # HYPERPARAMETERS ----------\n",
        "    batch_size = 64\n",
        "    block_size = 256\n",
        "    max_steps = 2500\n",
        "    learning_rate = 3e-4\n",
        "    eval_iters = 200\n",
        "    n_layer = 6\n",
        "    n_head = 6\n",
        "    n_emb = 498 # Must be multiple of n_head\n",
        "    dropout = 0.2\n",
        "    max_vocab_size = 350\n",
        "    # ------------------------\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "torch.manual_seed(MANUAL_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA_VwneC0dKa"
      },
      "source": [
        "## Preprocess dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuUceJBf0dKb",
        "outputId": "9c66ba01-8cb1-45ba-c71e-50bdc8e3b872"
      },
      "outputs": [],
      "source": [
        "if DATASET:\n",
        "    with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "\n",
        "    # Filter messages\n",
        "    messages = [line[line.index('-')+2:] for line in lines if not(\"<Media omitted>\" in line or \"<This message was edited>\" in line) and \"-\" in line] # Remove date\n",
        "\n",
        "    # Join the messages with new line characters\n",
        "    text = \"\".join(messages)\n",
        "    print(\"Length of dataset in characters: \", len(text))\n",
        "    print(text[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9m2yuM60dKc",
        "outputId": "60f80193-aba1-40e8-8508-ccff0fee9e96"
      },
      "outputs": [],
      "source": [
        "# Check all the unique characters in dataset\n",
        "if not LOAD:\n",
        "    chars = sorted(list(set(text)))\n",
        "    vocab_size = len(chars)\n",
        "    print(\"\".join(chars))\n",
        "    print(\"Vocabulary size: \", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgCUJpES3yX6",
        "outputId": "71f0bb2e-922f-4835-f557-1f25941626e8"
      },
      "outputs": [],
      "source": [
        "if not LOAD:\n",
        "  # Filter vocabulary\n",
        "  for c in \"Â¡Â¢Â£Â¥Â§Â©ÂªÂ¬Â´Â¶Â¿Ã„Ã†Ã‡Ã‹Ã–Ã¤Ã¦Ã¯Ã¶Ã¼Ã½ÄŒÄ˜ÄªÄ®Ä¯Å‹Å’Å“ÆƒÆÇÉÉ”ÉŸÉ¯É¹É¾Ê‡ÊŒÊ˜ÊË£Ë¥Î”Î£Î¨Î©Î«Î°Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î»Î¼Î¾Ï€ÏÏƒÏ„Ï†Ï‰ĞĞ¡Ğ²ĞµĞºĞ»Ğ¾Ğ¿ÑÑ‚Ô€à«±à¯¹á´‰á½½á¾á¾§á¾­â€‹â€â€“â€œâ€â€¢â€¦â€½â‰â°â´â·â¸â¹âºâ»â½â¾â¿â‚‘â‚’â‚“â‚›â‚¬â‚¯âƒ£â…‘â…·â†˜â†©â†¶â‡âˆˆâˆšâˆâˆ­âˆ®âˆ°â‰ âŒšâŒ¨â±â–«â–¶â—»â—½â˜â˜•â˜ â˜®â˜¯â˜¹â˜ºâ™€â™‚â™“â™Ÿâ™»âš”âš•âš›âš âš¡â›â›”â›ªâ›²âœˆâœ‹âœŒâœâœâœ“âœ¨â„âŒâ“â¤â¡â°â¬†â¬‡â¬›â¬œï·¼ï·½ï¸ï¼šï¿½ğŸ€„ğŸƒğŸ…°ğŸ…±ğŸ…¿ğŸ†“ğŸ†–ğŸ†—ğŸ†™ğŸ‡¦ğŸ‡§ğŸ‡©ğŸ‡ªğŸ‡¬ğŸ‡­ğŸ‡®ğŸ‡±ğŸ‡²ğŸ‡³ğŸ‡µğŸ‡·ğŸ‡¸ğŸ‡ºğŸ‡»ğŸ‡¿ğŸŒˆğŸŒŠğŸŒŒğŸŒğŸŒğŸŒšğŸŒğŸŒŸğŸŒ¤ğŸŒ«ğŸŒ¬ğŸŒ®ğŸŒ³ğŸŒ´ğŸŒ¸ğŸŒ½ğŸŒ¾ğŸ‚ğŸƒğŸ„ğŸ‡ğŸŒğŸğŸ”ğŸğŸ¢ğŸ£ğŸ¤ğŸ«ğŸ¬ğŸ³ğŸµğŸ½ğŸ¿ğŸ€ğŸğŸƒğŸ…ğŸˆğŸ‰ğŸŠğŸ“ğŸ–ğŸ™ğŸğŸ£ğŸ¤ğŸ¥ğŸ¬ğŸ²ğŸ´ğŸ¶ğŸ·ğŸ¸ğŸ¼ğŸ¾ğŸƒğŸ…ğŸğŸ”ğŸ£ğŸ¥ğŸ­ğŸ³ğŸ´ğŸ¸ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ€ğŸğŸ‚ğŸƒğŸ„ğŸ…ğŸ†ğŸˆğŸ‰ğŸŒğŸğŸ‘ğŸ’ğŸ”ğŸ•ğŸ–ğŸ—ğŸ˜ğŸ™ğŸ›ğŸğŸ ğŸ¢ğŸ£ğŸ¦ğŸ§ğŸ¨ğŸªğŸ«ğŸ­ğŸ®ğŸ¯ğŸ°ğŸ±ğŸ²ğŸ´ğŸµğŸ¶ğŸ·ğŸ¹ğŸºğŸ»ğŸ¼ğŸ½ğŸ¾ğŸ¿ğŸ‘ğŸ‘ƒğŸ‘„ğŸ‘…ğŸ‘‡ğŸ‘ˆğŸ‘‰ğŸ‘ŠğŸ‘‹ğŸ‘ŒğŸ‘ğŸ‘“ğŸ‘”ğŸ‘˜ğŸ‘£ğŸ‘¥ğŸ‘¦ğŸ‘§ğŸ‘¨ğŸ‘©ğŸ‘®ğŸ‘¯ğŸ‘°ğŸ‘³ğŸ‘´ğŸ‘µğŸ‘·ğŸ‘¸ğŸ‘¹ğŸ‘ºğŸ‘»ğŸ‘¼ğŸ‘½ğŸ‘¾ğŸ‘¿ğŸ’€ğŸ’ƒğŸ’…ğŸ’†ğŸ’‡ğŸ’‰ğŸ’‹ğŸ’ğŸ’šğŸ’œğŸ’¤ğŸ’¥ğŸ’§ğŸ’¨ğŸ’©ğŸ’ªğŸ’«ğŸ’®ğŸ’°ğŸ’±ğŸ’¸ğŸ’ºğŸ’»ğŸ“ˆğŸ“‰ğŸ“–ğŸ“œğŸ“ğŸ“ğŸ“§ğŸ“¸ğŸ“½ğŸ”ğŸ”ŠğŸ”‹ğŸ”ŒğŸ”¥ğŸ”§ğŸ”¨ğŸ”ªğŸ”«ğŸ”¬ğŸ”®ğŸ”µğŸ•ŠğŸ•ğŸ•ğŸ•µğŸ•¶ğŸ•¹ğŸ•ºğŸ–ğŸ–¥ğŸ—œğŸ—¡ğŸ—£ğŸ—¿ğŸ˜€ğŸ˜ğŸ˜ƒğŸ˜„ğŸ˜…ğŸ˜†ğŸ˜‡ğŸ˜ˆğŸ˜‰ğŸ˜ŠğŸ˜‹ğŸ˜ŒğŸ˜ğŸ˜ğŸ˜‘ğŸ˜’ğŸ˜“ğŸ˜”ğŸ˜•ğŸ˜–ğŸ˜—ğŸ˜˜ğŸ˜šğŸ˜›ğŸ˜œğŸ˜ğŸ˜ğŸ˜ ğŸ˜£ğŸ˜¤ğŸ˜¦ğŸ˜§ğŸ˜¨ğŸ˜©ğŸ˜ªğŸ˜«ğŸ˜¬ğŸ˜²ğŸ˜³ğŸ˜´ğŸ˜µğŸ˜¶ğŸ˜·ğŸ˜¹ğŸ˜ºğŸ˜¾ğŸ˜¿ğŸ™€ğŸ™ƒğŸ™…ğŸ™‡ğŸ™ˆğŸ™‰ğŸ™ŠğŸ™‹ğŸ™ŒğŸ™ğŸ™ğŸ™ğŸš€ğŸšğŸš‹ğŸšŒğŸšğŸš’ğŸš“ğŸš—ğŸš˜ğŸš™ğŸššğŸš¢ğŸš¨ğŸš«ğŸš¬ğŸš¯ğŸš²ğŸš³ğŸš´ğŸšµğŸš·ğŸš¸ğŸš¿ğŸ›‚ğŸ›ƒğŸ›ŒğŸ›‘ğŸ› ğŸ›¡ğŸ›£ğŸ›¸ğŸŸ¡ğŸŸ¢ğŸ¤ŒğŸ¤ğŸ¤ğŸ¤‘ğŸ¤•ğŸ¤–ğŸ¤—ğŸ¤˜ğŸ¤™ğŸ¤šğŸ¤ğŸ¤ğŸ¤ ğŸ¤¡ğŸ¤¢ğŸ¤¤ğŸ¤¥ğŸ¤¦ğŸ¤§ğŸ¤«ğŸ¤¬ğŸ¤­ğŸ¤®ğŸ¤°ğŸ¤±ğŸ¤²ğŸ¤·ğŸ¤¸ğŸ¥ğŸ¥‚ğŸ¥ŠğŸ¥ŒğŸ¥–ğŸ¥™ğŸ¥šğŸ¥ğŸ¥¢ğŸ¥¤ğŸ¥¦ğŸ¥©ğŸ¥¬ğŸ¥±ğŸ¥´ğŸ¥µğŸ¥¶ğŸ¥·ğŸ¥¸ğŸ¥¹ğŸ¥ºğŸ¥½ğŸ¥¿ğŸ¦ğŸ¦ƒğŸ¦…ğŸ¦†ğŸ¦‰ğŸ¦‹ğŸ¦ŒğŸ¦ğŸ¦’ğŸ¦“ğŸ¦•ğŸ¦–ğŸ¦—ğŸ¦˜ğŸ¦™ğŸ¦œğŸ¦ğŸ¦ŸğŸ¦¡ğŸ¦£ğŸ¦¤ğŸ¦¥ğŸ¦¦ğŸ¦«ğŸ¦¬ğŸ¦®ğŸ¦¯ğŸ¦²ğŸ¦´ğŸ¦¶ğŸ¦·ğŸ¦¹ğŸ¦ºğŸ¦»ğŸ¦¼ğŸ¦¾ğŸ¦¿ğŸ§‚ğŸ§„ğŸ§…ğŸ§ˆğŸ§‰ğŸ§ŒğŸ§ğŸ§‘ğŸ§“ğŸ§”ğŸ§˜ğŸ§™ğŸ§šğŸ§œğŸ§ŸğŸ§ ğŸ§«ğŸ§±ğŸ§¼ğŸ§¿ğŸ©³ğŸ©¸ğŸ©»ğŸªğŸª†ğŸªğŸª”ğŸªšğŸª›ğŸªğŸª¢ğŸª£ğŸª¤ğŸª¦ğŸª§ğŸª¨ğŸª¬ğŸª°ğŸª±ğŸª²ğŸª³ğŸªµğŸª·ğŸª¹ğŸªºğŸ«€ğŸ«ğŸ«‚ğŸ«ƒğŸ«’ğŸ«”ğŸ«•ğŸ«–ğŸ«—ğŸ«˜ğŸ« ğŸ«¡ğŸ«¢ğŸ«£ğŸ«¤ğŸ«¥ğŸ«§ğŸ«°ğŸ«±ğŸ«³ğŸ«´ğŸ«µğŸ«¶\":\n",
        "    chars.remove(c)\n",
        "    text = text.replace(c, \"\")\n",
        "\n",
        "  vocab_size = len(chars)\n",
        "  print(\"\".join(chars))\n",
        "  print(\"Final vocabulary size: \", vocab_size)\n",
        "  print(\"Final dataset size in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa6G8764VvdW"
      },
      "source": [
        "## Tokenizing with Byte Pair Encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkeJShyzWQ7h"
      },
      "outputs": [],
      "source": [
        "tokens = text.encode('utf-8')\n",
        "tokens = list(map(int, tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJPc2odTIhA6",
        "outputId": "cdd67d38-5829-4314-afd8-ccf128707447"
      },
      "outputs": [],
      "source": [
        "len(set(text)), len(set(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RuhVJCmVyPq",
        "outputId": "d7fc730b-6459-4bbd-a8d4-9afcd89d0676"
      },
      "outputs": [],
      "source": [
        "def get_pair_frequency(ids):\n",
        "    pair_frequency = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        pair_frequency[pair] = pair_frequency.get(pair, 0) + 1\n",
        "    return pair_frequency\n",
        "\n",
        "pair_frequency = get_pair_frequency(tokens)\n",
        "print(pair_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueUtAl7uXraN",
        "outputId": "15b9e4d3-e660-4f1f-a2ec-f2f873e7586c"
      },
      "outputs": [],
      "source": [
        "top_pair = max(pair_frequency, key=pair_frequency.get)\n",
        "top_pair, chr(top_pair[0]), chr(top_pair[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_mBvehzX2xp"
      },
      "outputs": [],
      "source": [
        "# Merges the top_pair in a single new token\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpJQiN_pZNcs"
      },
      "outputs": [],
      "source": [
        "vocab_size = 256\n",
        "new_vocab_size = vocab_size\n",
        "ids = tokens\n",
        "merges = {} # (int, int) -> int\n",
        "while (new_vocab_size < max_vocab_size):\n",
        "  pair_frequency = get_pair_frequency(ids)\n",
        "  top_pair = max(pair_frequency, key=pair_frequency.get)\n",
        "  ids = merge(ids, top_pair, new_vocab_size)\n",
        "  merges[top_pair] = new_vocab_size\n",
        "  new_vocab_size += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qucgWRkTaawS"
      },
      "source": [
        "### Compression stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G5tXR3PadeH",
        "outputId": "18849129-71b8-41d6-cc6d-b3694a30e7ac"
      },
      "outputs": [],
      "source": [
        "print(\"Tokens length: \", len(tokens))\n",
        "print(\"ids length: \", len(ids))\n",
        "print(f\"Compression ratio: {len(tokens)/len(ids):.2f}X\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URkGbNbMb7aB"
      },
      "source": [
        "## Decode and Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eLy2VizcJ_U"
      },
      "outputs": [],
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (c1, c2), idx in merges.items():\n",
        "  # print(c1, \"\", c2)\n",
        "  vocab[idx] = vocab[c1] + vocab[c2]\n",
        "vocab_size = new_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUfmb7NB0dKd",
        "outputId": "2eeaf425-3aca-4042-d31e-ab94bef735d9"
      },
      "outputs": [],
      "source": [
        "def decode(ids):\n",
        "  # given ids (list of integers) return python string\n",
        "  tks = b\"\".join([vocab[idx] for idx in ids])\n",
        "  return tks.decode('utf-8', errors=\"replace\")\n",
        "\n",
        "print(decode([226, 156, 133]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS-yqe8AgYIZ",
        "outputId": "b7dcfba5-1528-497b-d51b-1cf0f754ff64"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "  tks = list(text.encode(\"utf-8\"))\n",
        "  while len(tks) >= 2:\n",
        "    pair_frequency = get_pair_frequency(tks)\n",
        "    pair = min(pair_frequency, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break # nothing else can be merged\n",
        "    idx = merges[pair]\n",
        "    tks = merge(tks, pair, idx)\n",
        "  return tks\n",
        "\n",
        "print(encode(\"âœ…\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x3BA-Eh0dKe",
        "outputId": "71560a23-751b-492f-9957-d1efe86958ba"
      },
      "outputs": [],
      "source": [
        "print(encode(\"Hello, world!\"))\n",
        "print(decode(encode(\"Hello, world!\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shy0REXx0dKf"
      },
      "source": [
        "## Tokenize the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9l9U7t-0dKg",
        "outputId": "d026f858-dbf5-48ff-ad69-4cd8dc7b284a"
      },
      "outputs": [],
      "source": [
        "if DATASET:\n",
        "    data = torch.tensor(encode(text), dtype=torch.long)\n",
        "    print(data.shape, data.dtype)\n",
        "    print(data[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm60-F980dKh"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LQDWSs80dKh"
      },
      "outputs": [],
      "source": [
        "if DATASET:\n",
        "    n = int(0.9 *len(data))\n",
        "    train_data = data[:n]\n",
        "    val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNK5R8v40dKi"
      },
      "source": [
        "## Get chunck of data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erEbxv0D0dKi"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))  # choose random starting points for each sequence in the batch\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwlx2nlq0dKi"
      },
      "source": [
        "## Head of self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjhW_KAz0dKj"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"One head of self attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # not a parameter of the model\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "\n",
        "        # compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T) # normalize (\"scaled attention\")\n",
        "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform weighted agregation of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S7TKev50dKj"
      },
      "source": [
        "## Multi Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHVaNTBE0dKj"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multiples heas of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_emb, n_emb)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uikguZ6x0dKk"
      },
      "source": [
        "## Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z3vaQFi0dKk"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\"A simple linear layer with a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_emb):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_emb, 4*n_emb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_emb, n_emb) ,# projection layer\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-02AsQdw0dKk"
      },
      "source": [
        "## Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KdkOOdR0dKl"
      },
      "outputs": [],
      "source": [
        "class  Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self,n_emb,n_head):\n",
        "        # n_emb: embedding size\n",
        "        # n_head: number of heads\n",
        "        super().__init__()\n",
        "        head_size = n_emb // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_emb)\n",
        "        self.ln1 = nn.LayerNorm(n_emb)\n",
        "        self.ln2 = nn.LayerNorm(n_emb)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDE4GKtt0dKl"
      },
      "source": [
        "## Language model\n",
        "\n",
        "- B: Batch_size\n",
        "- T: Length of the input sequence\n",
        "- C: Number of features per token (size of the embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PlRQ6060dKl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize a matrix of shape (vocab_size, vocab_size) with  random values\n",
        "        # that will be optimized during training\n",
        "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_emb)\n",
        "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_emb)\n",
        "        self.blocks = nn.Sequential(*[Block(n_emb, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_emb) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_emb)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T,device=device)) # (T, n_emb)\n",
        "        x = tok_emb + pos_emb # (B, T, n_emb)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "            B,T,C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B,T) array of indeces in the current context\n",
        "        # the goal is to get (B, T+1), (B, T+2), ... (B, T+max_new_tokens )\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_crop = idx[:, -block_size:]\n",
        "\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_crop)\n",
        "\n",
        "            # focus only on the last time step (the logit of the last token in the context)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "\n",
        "            # softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat([idx, idx_next], dim=1)\n",
        "\n",
        "        return idx\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XFJ560V0dKm"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeE4LJWt0dKm"
      },
      "outputs": [],
      "source": [
        "if LOAD:\n",
        "    model = LanguageModel()\n",
        "    # Load the model state dictionary\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device(device)))\n",
        "else:\n",
        "    m = LanguageModel()\n",
        "    model = m.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwqqC5LM0dKm"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGPwS8KF0dKm"
      },
      "outputs": [],
      "source": [
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYCXJkQm0dKn",
        "outputId": "0410882c-c616-4c2f-f44c-2921a3b71504"
      },
      "outputs": [],
      "source": [
        "if DATASET:\n",
        "    for i in range(2500):\n",
        "\n",
        "        # sample batch\n",
        "        xb, yb = get_batch('train')\n",
        "\n",
        "        # evaluate the loss\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if(i % (max_steps//20) == 0):\n",
        "            print(f\"{i/max_steps*100:3.3}% complete - loss: {loss.item()}\")\n",
        "\n",
        "print(f\"100.0% complete - loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bowREyp0dKn"
      },
      "source": [
        "## Better estimate the loss\n",
        "\n",
        "Since the loss is calculated for a single batch, it may not representate well the loss of the whole data.\n",
        "\n",
        "So to estimate we'll calculate the loss for many batches and then return the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7omOqohs0dKo",
        "outputId": "72b90647-a5c7-4c99-bdc8-24811f3446bb"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(eval_iters=500):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            xb, yb = get_batch(split)\n",
        "            logits, loss = model(xb, yb)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "if DATASET:\n",
        "    print(estimate_loss(eval_iters))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhjnfaPN0dKo"
      },
      "source": [
        "## Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ19RXIJ0dKp",
        "outputId": "5c4fc5a2-bcab-40e5-c8fb-cce83b6c2d61"
      },
      "outputs": [],
      "source": [
        "prompt = \"BÃ¡rbara Bueno: A\"\n",
        "prompt_encoded = encode(prompt)\n",
        "idx = torch.tensor([prompt_encoded], dtype=torch.long, device=device) # Contains the character corresponding to index 0 (newline)\n",
        "# idx.shape\n",
        "print(decode(model.generate(idx, max_new_tokens=500)[0].tolist())) # decode the first batch of the generated completions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLttoMcp0dKp"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCENlqck0dKp"
      },
      "outputs": [],
      "source": [
        "SAVE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFQ6_7Z80dKp"
      },
      "outputs": [],
      "source": [
        "if SAVE:\n",
        "    torch.save(m.state_dict(), save_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voDaYiPX0dKq"
      },
      "outputs": [],
      "source": [
        "if SAVE:\n",
        "    import json\n",
        "    config = {\n",
        "        \"batch_size\": batch_size,\n",
        "        \"block_size\": block_size,\n",
        "        \"max_steps\": max_steps,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"eval_iters\": eval_iters,\n",
        "        \"n_emb\": n_emb,\n",
        "        \"n_layer\": n_layer,\n",
        "        \"n_head\": n_head,\n",
        "        \"dropout\": dropout,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"stoi\": stoi,\n",
        "        \"itos\": itos\n",
        "    }\n",
        "\n",
        "\n",
        "    # Salve o dicionÃ¡rio em um arquivo JSON\n",
        "    with open(save_config_path, 'w') as f:\n",
        "        json.dump(config, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
